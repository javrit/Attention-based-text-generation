{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based text generation\n",
    "\n",
    "### Introduction\n",
    "\n",
    "**Why is Text Generation So Hard for AI?**\n",
    "\n",
    "Text generation is one of the toughest challenges in AI. Unlike humans, who can easily recall information, pick up on context, and make inferences, machines struggle to stay coherent and consistent over long sequences. Keeping track of the bigger picture while generating meaningful text is a difficult task for AI.\n",
    "\n",
    "**RNNs: A Game-Changer (But Not Perfect)**\n",
    "\n",
    "One of the key breakthroughs in text generation was the introduction of Recurrent Neural Networks (RNNs). They revolutionized Natural Language Processing (NLP) by making it possible to handle sequence-based tasks like **machine translation, text summarization, and even creative writing.\n",
    "\n",
    "**Why are RNNs useful?**  \n",
    "\n",
    "- They **process words sequentially**, meaning they understand how words relate to each other in order.  \n",
    "- They **capture dependencies** between words, which helps maintain logical flow.  \n",
    "\n",
    "Despite their advantages, RNNs face serious limitations. The biggest issue is the vanishing gradient problem, which makes it hard for the network to remember information from earlier in a long sequence. This means the longer the text, the harder it is for the model to stay relevant and coherent. Even improved versions like LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) struggle with long-range dependencies.\n",
    "\n",
    "**Attention Mechanisms: The Game-Changer**  \n",
    "\n",
    "To solve this issue, attention mechanisms were introduced. These allow models to dynamically focus on the most relevant parts of the input sequence instead of treating all words equally. This breakthrough was a major milestone in text generation, leading to much better results.\n",
    "\n",
    "Some key papers that introduced attention-based text generation include:  \n",
    "- [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044) – A model that applied attention to image captioning, allowing it to describe images in a more natural way.  \n",
    "- Data-to-Text Generation with Attention Recurrent Units – A system that used attention to generate summaries of NBA games from game statistics.\n",
    "\n",
    "Thanks to attention mechanisms, text generation models are now far more powerful, making them essential for applications like chatbots, summarization, and machine translation.\n",
    "\n",
    "---\n",
    "\n",
    "### What Will We Cover?\n",
    "In this notebook, we will:\n",
    "- Introduce attention mechanisms.\n",
    "- Explore different types of input data for text generation:\n",
    "  - Text: Machine translation and language modeling.\n",
    "  - Images: Generating captions using vision-based models (e.g., [\"Show, Attend and Tell\"](https://arxiv.org/pdf/1502.03044)).\n",
    "\n",
    "Let's get started !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'notebook_tuto_captioning' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/javrit/notebook_tuto_captioning.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dive into Attention Mechanisms\n",
    "\n",
    "### Introduction\n",
    "Attention is a technique in machine learning that helps models figure out which parts of a sequence are the most important. In natural language processing (NLP), this means assigning different weights to words in a sentence based on their relevance. More broadly, attention helps map connections between different parts of a sequence—whether it’s a few words or millions of tokens.\n",
    "\n",
    "This idea comes from how humans pay attention. Instead of treating all words equally, attention lets a model focus on the right parts at the right time. It was created to fix a big problem with Recurrent Neural Networks (RNNs)—they tend to focus too much on recent information while forgetting earlier details. Unlike RNNs, attention allows models to access any word in a sentence directly, making it much better at handling long-range dependencies.\n",
    "\n",
    "In simple terms, attention helps a model zoom in on the most important parts of an input. Whether it's a sentence, an image, or even a game summary, attention figures out what matters most. For example, in machine translation, attention assigns different importance levels to words, making sure the right ones are considered at each step of translation.\n",
    "\n",
    "![Attention mechanism](img/attention.png)\n",
    "*https://datascience.stackexchange.com/questions/66913/how-does-attention-mechanism-learn*\n",
    "\n",
    "Let's have a quick look of how it mathematically works.\n",
    "\n",
    "### Mathematical Explanation of Attention \n",
    "\n",
    "Attention mechanisms compute a **weighted sum of values** based on the similarity between **queries** and **keys**, allowing a model to dynamically focus on the most relevant parts of an input sequence. This process consists of three main steps:\n",
    "\n",
    "---\n",
    "\n",
    "##### **1. Score Calculation (Alignment Scores)**\n",
    "Each token in the input sequence is represented by three vectors:  \n",
    "- **Query (Q)** – Represents the current token looking for relevant information.  \n",
    "- **Key (K)** – Represents all tokens in the input sequence, serving as references.  \n",
    "- **Value (V)** – Contains the actual token representations that will be combined to form the final output.  \n",
    "\n",
    "The attention mechanism first computes a **score** for each token pair to determine its relevance. In the **encoder-decoder attention** setting, this score measures how well an input token $h_i$ aligns with a decoder state $s_t$:\n",
    "\n",
    "$$\n",
    "    e_{ti} = f(s_t, h_i)\n",
    "$$\n",
    "\n",
    "where $f$ is a scoring function, often defined as:  \n",
    "- **Dot-product**: $e_{ti} = s_t^T h_i$  \n",
    "- **Additive (Bahdanau)**: $e_{ti} = v^T \\tanh(W_s s_t + W_h h_i)$  \n",
    "\n",
    "In **self-attention (used in Transformers)**, the score is computed using **dot-product similarity** between queries and keys:\n",
    "\n",
    "$$\n",
    "    e_{ti} = Q K^T\n",
    "$$\n",
    "\n",
    "where $Q = X W_Q$ and $K = X W_K$ are obtained from the input sequence $X$.\n",
    "\n",
    "---\n",
    "\n",
    "##### **2. Weight Assignment (Softmax Normalization)**\n",
    "The raw alignment scores are then normalized using the **softmax function**, ensuring that they sum to 1 and act as attention weights:\n",
    "\n",
    "$$\n",
    "    \\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_{j} \\exp(e_{tj})}\n",
    "$$\n",
    "\n",
    "In **self-attention**, we add a scaling factor to stabilize training:\n",
    "\n",
    "$$\n",
    "    \\alpha_{ti} = \\text{softmax} \\left(\\frac{Q K^T}{\\sqrt{d_k}} \\right)\n",
    "$$\n",
    "\n",
    "where $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "---\n",
    "\n",
    "##### **3. Context Vector Computation**\n",
    "Finally, the attention mechanism computes a **weighted sum of values** to create a **context vector**:\n",
    "\n",
    "$$\n",
    "    c_t = \\sum_{i} \\alpha_{ti} h_i\n",
    "$$\n",
    "\n",
    "In self-attention, this is expressed as:\n",
    "\n",
    "$$\n",
    "    \\text{Attention}(Q, K, V) = \\alpha V\n",
    "$$\n",
    "\n",
    "where $V = X W_V$ represents the values that encode the actual input token information.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary**\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"img/attentionkey.png\" width=\"500\"></td>\n",
    "        <td><img src=\"img/selfqttention.png\" width=\"500\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "*https://www.linkedin.com/feed/update/urn:li:activity:7113403245703663616/*\n",
    "- The **score function** determines how relevant each token is to the current step.\n",
    "- **Softmax assigns attention weights**, controlling focus on different tokens.\n",
    "- **A weighted sum of values produces the final attention output**, dynamically adapting to context.\n",
    "\n",
    "This mechanism enables models to effectively capture long-range dependencies.\n",
    "\n",
    "OK ! Now that we have seen a small reminder on how attention works, let's jump into practice !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text given a context sequence of text\n",
    "\n",
    "In this section, we will implement a model that allows the generation of text with a context sequence of text. It is a really basic example to see how it works. The goal is to generate answers to common french sentences (salut ça va ?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Set the device (use GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We create a simple character-level vocabulary which includes:\n",
    "- **Special tokens:** `<pad>`, `<sos>` (start-of-sequence), and `<eos>` (end-of-sequence)\n",
    "- **Letters:** A space followed by the letters `a` to `z` (all in lowercase)\n",
    "\n",
    "Next, we define two functions:\n",
    "- `sentence_to_indices`: Converts a sentence into a list of indices by mapping each character to its index (adding `<sos>` at the beginning and `<eos>` at the end).\n",
    "- `indices_to_sentence`: Reconstructs a sentence from a sequence of indices while ignoring the special tokens.\n",
    "\n",
    "Finally, we create a small dataset consisting of pairs (context → target sentence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30\n"
     ]
    }
   ],
   "source": [
    "# Define a simple character-level vocabulary\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "letters = list(\" abcdefghijklmnopqrstuvwxyz\")  # Note: space is included\n",
    "vocab = special_tokens + letters\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Create mapping dictionaries\n",
    "word2index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Function to convert a sentence to a list of indices\n",
    "def sentence_to_indices(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    indices = [word2index[\"<sos>\"]]\n",
    "    for ch in sentence:\n",
    "        if ch in word2index:  # Keep only known characters\n",
    "            indices.append(word2index[ch])\n",
    "    indices.append(word2index[\"<eos>\"])\n",
    "    return indices\n",
    "\n",
    "# Function to convert a sequence of indices back into a sentence\n",
    "def indices_to_sentence(indices):\n",
    "    words = [index2word[idx] for idx in indices if idx not in [word2index[\"<sos>\"], word2index[\"<eos>\"], word2index[\"<pad>\"]]]\n",
    "    return \"\".join(words)\n",
    "\n",
    "# Example dataset (pairs: context -> target sentence)\n",
    "data_pairs = [    \n",
    "    (\"Ça fait un bail\",\"Je suis trop content(e) de te voir\"),\n",
    "    ( \"Ça faisait longtemps !\", \"Ça fait plaisir de te voir\"),\n",
    "    (\"Comment tu vas depuis la dernière fois ?\", \"Ça va.\"),\n",
    "    (\"Tu vas bien ?\", \"Je vais bien.\"),\n",
    "    (\"ça va ?\", \"Je pète la forme\"),\n",
    "    (\"Comment tu vas ? \",\"Bof, on fait avec.\"),          \n",
    "    (\"Comment ça va ?\", \"C’est pas la grande forme.\"),   \n",
    "    (\"La forme ?\",\"Je suis sur les rotules.\"),\n",
    "    (\"Quoi de neuf ?\", \"Rien de nouveau.\"),\n",
    "    (\"Qu’est-ce que tu racontes de beau ?\", \"La routine.\"),\n",
    "    (\"Comment s’est passée ta semaine ?\", \"J’étais débordé(e).\"),\n",
    "    (\"Ça va la famille ?\", \"Tout le monde va bien\"),\n",
    "    (\"Il fait beau chez toi ?\", \"C’est vraiment une belle journée.\"),\n",
    "    (\"T’as des plans ce weekend ?\", \"J’ai un repas avec des amis samedi soir.\"),\n",
    "    (\"T’as entendu la dernière ?\", \"Oui, c’est complètement fou !\"),\n",
    "    (\"T’es allé(e) au cinéma dernièrement ?\", \"J’ai vu le nouveau Batman au ciné. C’était super long, mais j’ai bien aimé.\"),\n",
    "    (\"Tu vas prendre des vacances prochainement ?\", \"Oui, je vais prendre des vacances bientôt.\"),\n",
    "    (\"On se voit bientôt ?\", \"À bientôt !\"),\n",
    "    (\"Salut !\", \"Salut !\")\n",
    "]\n",
    "\n",
    "\n",
    "# Convert the sentences into sequences of indices\n",
    "data = []\n",
    "for inp, target in data_pairs:\n",
    "    input_indices = sentence_to_indices(inp)\n",
    "    target_indices = sentence_to_indices(target)\n",
    "    data.append((input_indices, target_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The encoder is composed of an embedding layer followed by an LSTM. An embedding layer is a special type of neural network layer used to transform categorical data (like words in text) into dense numerical vectors. As a reminder : LSTM stands for **Long Short-Term Memory**. It is a special type of recurrent neural network (RNN) designed to effectively capture long-term dependencies in sequential data while mitigating the vanishing gradient problem common in traditional RNNs.\n",
    "\n",
    "It receives a sequence of indices (the context sentence) as input and produces:\n",
    "- The LSTM outputs for each time step\n",
    "- The final hidden state and cell state\n",
    "\n",
    "These states are later used by the decoder.\n",
    "\n",
    "\n",
    "\n",
    "![lstm](img/LSTM.png) \n",
    "\n",
    "*Source : [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044)*\n",
    "\n",
    "Key characteristics of LSTMs include:\n",
    "\n",
    "- **Memory Cells:** They maintain a cell state that acts as a memory, carrying relevant information throughout the processing of a sequence.\n",
    "- **Gating Mechanisms:** LSTMs use gates (input, forget, and output gates) to control the flow of information. These gates decide which information is important to keep, update, or discard over time.\n",
    "- **Handling Long-Term Dependencies:** By managing information over long sequences, LSTMs are well-suited for tasks such as language modeling, text generation, and time-series prediction.\n",
    "\n",
    "In our model, the LSTM is used to process the sequence of embedded characters, learning the patterns and dependencies in the input text to generate coherent output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #The embedding maps each token index to a dense vector of a fixed\n",
    "        #size (embedding_dim here). It converts these indices (discrete) into continuous vectors that capture semantic between tokens\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        # input_seq shape: (batch, seq_len)\n",
    "        embedded = self.embedding(input_seq)             # (batch, seq_len, embedding_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)        # outputs: (batch, seq_len, hidden_dim)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-based Decoder\n",
    "\n",
    "The decoder generates the output sequence character-by-character.  \n",
    "At each time step, it uses:\n",
    "- The current input token (embedded)\n",
    "- A context vector computed using an attention mechanism (dot-product between the decoder hidden state and the encoder outputs)\n",
    "\n",
    "The context vector is computed by taking a weighted sum of the encoder outputs, where the weights are determined by the attention scores.  \n",
    "This context is concatenated with the embedding and fed into the LSTM.  \n",
    "Finally, the LSTM output and the context are combined to produce a distribution over the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Concatenate the embedding and the context vector before feeding into the LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        # Output layer combines the LSTM output and the context to produce vocabulary distribution\n",
    "        self.out = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "    \n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
    "        # input_token shape: (batch) the current token index\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (batch, 1, embedding_dim)\n",
    "      \n",
    "        # --- Dot-product Attention Mechanism ---\n",
    "        # hidden shape: (num_layers, batch, hidden_dim); we use the first layer (shape: (batch, hidden_dim))\n",
    "        decoder_hidden = hidden[0]           # (batch, hidden_dim)\n",
    "        # encoder_outputs shape: (batch, seq_len, hidden_dim)\n",
    "        # Compute attention scores (dot product)\n",
    "        attn_scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)  # (batch, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)       # (batch, seq_len)\n",
    "      \n",
    "        # Compute context vector as the weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch, 1, hidden_dim)\n",
    "      \n",
    "        # Concatenate the embedding and the context\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)     # (batch, 1, embedding_dim + hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))  # output: (batch, 1, hidden_dim)\n",
    "      \n",
    "        # Combine LSTM output and context for the final prediction\n",
    "        output = output.squeeze(1)    # (batch, hidden_dim)\n",
    "        context = context.squeeze(1)  # (batch, hidden_dim)\n",
    "        combined = torch.cat((output, context), dim=1)  # (batch, 2*hidden_dim)\n",
    "        output = self.out(combined)   # (batch, vocab_size)\n",
    "        output = torch.log_softmax(output, dim=1)\n",
    "        return output, hidden, cell, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We train our model on a very small dataset consisting of a few (context → target) pairs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.0103\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "num_epochs = 50    # Small dataset \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Instantiate the models\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "decoder = AttentionDecoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.NLLLoss(ignore_index=word2index[\"<pad>\"])\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # For each data pair (input, target) in our dataset\n",
    "    for input_indices, target_indices in data:\n",
    "        # Convert lists of indices to tensors (batch size = 1)\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "        target_tensor = torch.tensor(target_indices, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        # Pass through the encoder\n",
    "        encoder_outputs, hidden, cell = encoder(input_tensor)\n",
    "        \n",
    "        # --- Pass through the decoder with teacher forcing ---\n",
    "        loss = 0\n",
    "        # The first input token for the decoder is always the <sos> token\n",
    "        decoder_input = torch.tensor([target_indices[0]], device=device)\n",
    "        # Loop over the target sequence (starting from the second token)\n",
    "        for t in range(1, len(target_indices)):\n",
    "            output, hidden, cell, attn_weights = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "            # The target token to predict\n",
    "            target_token = target_tensor[t].unsqueeze(0)  # shape (1)\n",
    "            loss += criterion(output, target_token)\n",
    "            # Teacher forcing: use the actual target as the next input\n",
    "            decoder_input = target_tensor[t].unsqueeze(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        # Average the loss over the sequence length\n",
    "        total_loss += loss.item() / (len(target_indices) - 1)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text\n",
    "\n",
    "The `generate_sentence` function takes a context string as input and generates a continuation of the text up to a maximum number of characters (or until the `<eos>` token is generated).  \n",
    "It works as follows:\n",
    "1. The context is passed through the encoder.\n",
    "2. The decoder is used to generate text character-by-character by selecting at each step the token with the highest probability (greedy decoding).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tu vas bien?\n",
      "Generated: je vais bien\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(context_sentence, max_length=20):\n",
    "    \"\"\"\n",
    "    Generates a sentence (target sentence) from a given context sentence.\n",
    "    \n",
    "    Parameters:\n",
    "      - context_sentence (str): The input context sentence.\n",
    "      - max_length (int): Maximum length of the generated sentence.\n",
    "    \n",
    "    Returns:\n",
    "      - A generated sentence (str), built character-by-character.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert the context sentence to indices and create a tensor\n",
    "        input_indices = sentence_to_indices(context_sentence)\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "        \n",
    "        # Pass the context through the encoder\n",
    "        encoder_outputs, hidden, cell = encoder(input_tensor)\n",
    "        \n",
    "        # Initialize the decoder with the <sos> token\n",
    "        decoder_input = torch.tensor([word2index[\"<sos>\"]], device=device)\n",
    "        generated_sentence = \"\"\n",
    "        \n",
    "        # Generate characters up to max_length or until <eos> is produced\n",
    "        for i in range(max_length):\n",
    "            output, hidden, cell, attn_weights = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "            # Select the token with the highest probability (greedy decoding)\n",
    "            topv, topi = output.topk(1)\n",
    "            next_token = topi.item()\n",
    "            if next_token == word2index[\"<eos>\"]:\n",
    "                break\n",
    "            generated_sentence += index2word[next_token]\n",
    "            # Use the predicted token as the next decoder input\n",
    "            decoder_input = torch.tensor([next_token], device=device)\n",
    "    \n",
    "    return generated_sentence\n",
    "\n",
    "# --- Test the generation function ---\n",
    "test_context = \"tu vas bien?\"  # Example context sentence, you can try other if you want\n",
    "print(\"Context:\", test_context)\n",
    "print(\"Generated:\", generate_sentence(test_context, max_length=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to try with other examples but it might not work well, because of how small the training dataset is. It was just an example to implement an attention mechanism in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that you know how it works, let's try a more concrete example. Because training might be very expensive, we will use the pre-trained model on this [github](https://github.com/ApurbaSengupta/Text-Generation.git). It is a model that generates a sentence using a context sentence, and that has been trained on 4 english novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Text-Generation' already exists and is not an empty directory.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/javrit/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/javrit/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: unidecode in /home/javrit/.local/lib/python3.10/site-packages (1.3.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/javrit/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ApurbaSengupta/Text-Generation.git\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines the model, using again attention and it is a good exercise to have a look at it and try to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import time, math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "use_cuda = False \n",
    "\n",
    "class TextGenerate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bi=True):\n",
    "        super(TextGenerate, self).__init__()\n",
    "        \n",
    "        # Define key hyperparameters\n",
    "        self.input_size = input_size  # Vocabulary size\n",
    "        self.hidden_size = hidden_size  # LSTM hidden layer size\n",
    "        self.output_size = output_size  # Output vocabulary size (same as input in text generation)\n",
    "        self.n_layers = n_layers  # Number of LSTM layers\n",
    "        self.bi = bi  # Boolean flag for bidirectional LSTM\n",
    "\n",
    "        # Word embedding layer: maps input indices to dense vector representations\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # LSTM layer: processes input embeddings into a hidden state\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=self.bi)\n",
    "        \n",
    "        # Fully connected layer: maps LSTM outputs to the output vocabulary\n",
    "        if self.bi:\n",
    "            self.decoder = nn.Linear(hidden_size * 2, output_size)  # For bidirectional LSTM\n",
    "        else:\n",
    "            self.decoder = nn.Linear(hidden_size, output_size)  # For unidirectional LSTM\n",
    "\n",
    "        # Another linear layer to refine the final output\n",
    "        self.out = nn.Linear(output_size, output_size)\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        - Embedding input words\n",
    "        - Passing through LSTM to generate context-aware representation\n",
    "        - Applying an attention mechanism\n",
    "        - Decoding the attention-weighted representation to predict the next word\n",
    "        \"\"\"\n",
    "        \n",
    "        # ---- Encoder ----\n",
    "        # Convert input token indices into dense embeddings\n",
    "        input = self.encoder(input.view(1, -1))  # Shape: (1, hidden_size)\n",
    "        input = self.dropout(input)  # Apply dropout for regularization\n",
    "\n",
    "        # Pass embeddings through LSTM\n",
    "        output, states = self.lstm(input.view(1, 1, -1), (hidden, cell))  # Shape: (1, 1, hidden_size)\n",
    "        output = output.permute(1, 0, 2)  # Permute to shape: (batch=1, seq_len=1, hidden_size)\n",
    "\n",
    "        # ---- Attention Mechanism ----\n",
    "        if self.bi:  # If using bidirectional LSTM\n",
    "            # Split the output into two parts (one for each direction)\n",
    "            out1, out2 = output[:, :, :self.hidden_size], output[:, :, self.hidden_size:]\n",
    "\n",
    "            # Get the last hidden states for both directions\n",
    "            h1, h2 = states[0][-2, :, :], states[0][-1, :, :]\n",
    "\n",
    "            # Compute attention weights for both directions\n",
    "            attn_wts_1 = F.softmax(torch.bmm(out1, h1.unsqueeze(2)).squeeze(2), dim=1)  # Shape: (batch, seq_len)\n",
    "            attn_wts_2 = F.softmax(torch.bmm(out2, h2.unsqueeze(2)).squeeze(2), dim=1)\n",
    "\n",
    "            # Compute attention-weighted context vectors\n",
    "            attn_1 = torch.bmm(out1.transpose(1, 2), attn_wts_1.unsqueeze(2)).squeeze(2)  # (batch, hidden_size)\n",
    "            attn_2 = torch.bmm(out2.transpose(1, 2), attn_wts_2.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "            # Concatenate attention vectors from both directions\n",
    "            attn = torch.cat((attn_1, attn_2), dim=1)  # Final attention output: (batch, hidden_size * 2)\n",
    "\n",
    "        else:  # If using unidirectional LSTM\n",
    "            h = states[0].squeeze(0)  # Last hidden state\n",
    "\n",
    "            # Compute attention weights\n",
    "            attn_wts = F.softmax(torch.bmm(output, h.unsqueeze(2)).squeeze(2), dim=1)  # Shape: (batch, seq_len)\n",
    "\n",
    "            # Compute attention-weighted context vector\n",
    "            attn = torch.bmm(output.transpose(1, 2), attn_wts.unsqueeze(2)).squeeze(2)  # Shape: (batch, hidden_size)\n",
    "\n",
    "        # ---- Decoder ----\n",
    "        # Apply the decoder to transform attention vector into output logits\n",
    "        output = self.decoder(attn)  # Shape: (batch, output_size)\n",
    "        output = self.dropout(output)  # Apply dropout\n",
    "        output = self.out(output)  # Final output layer\n",
    "\n",
    "        return output, states  # Return the predicted token logits and updated hidden state\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.bi:\n",
    "          return Variable(torch.zeros(self.n_layers*2, 1, self.hidden_size))\n",
    "        else:\n",
    "          return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "\n",
    "    def init_cell(self):\n",
    "        if self.bi:\n",
    "          return Variable(torch.zeros(self.n_layers*2, 1, self.hidden_size))\n",
    "        else:\n",
    "          return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "\n",
    "# turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    if use_cuda:\n",
    "      tensor = tensor.cuda()\n",
    "    return Variable(tensor)\n",
    "\n",
    "# generate text given context\n",
    "def generate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    model.load_state_dict(torch.load('Text-Generation/model_generate.pt', map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "    hidden = model.init_hidden()\n",
    "    cell = model.init_cell()\n",
    "\n",
    "    if use_cuda:\n",
    "      hidden = hidden.cuda()\n",
    "      cell = cell.cuda()\n",
    "\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str + \"\\n--------->\\n\"\n",
    "\n",
    "    # use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, states = model(prime_input[p], hidden, cell)\n",
    "\n",
    "        if use_cuda:\n",
    "          hidden, cell = states[0].cuda(), states[1].cuda()\n",
    "        else:\n",
    "          hidden, cell = states[0], states[1]\n",
    "\n",
    "    inp = prime_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, states = model(inp, hidden, cell)\n",
    "\n",
    "        if use_cuda:\n",
    "          output = output.cuda()\n",
    "          hidden, cell = states[0].cuda(), states[1].cuda()\n",
    "        else:\n",
    "          hidden, cell = states[0], states[1]\n",
    "\n",
    "        # sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 :\n",
      "\n",
      "\n",
      "The tumult of her mind, was now painfully great. She knew not how   to support herself, and from actual weakness sat down and cried for   half-an-hour. \n",
      "--------->\n",
      "Such a sun was visible things to see him to the struck as the\n",
      "end from plain Flask, however me that the crossess was and raising who calmed but the cratterly was soul so figure to have admires to\n",
      "be also. There did noble hungest down called the other side of thing all believe. And object of them whi \n",
      "\n",
      "Example 2 :\n",
      "\n",
      "\n",
      "To believe in things that you cannot. Let me illustrate. I heard once   of an American who so defined faith: 'that faculty which enables us to   believe things which we know to be untrue.' For one, I follow that man. \n",
      "--------->\n",
      "Not that his of\n",
      "think out it as you make these minutes, and it Is all this did, not medre bluption to be the remetion-most beat as if one there will feeling on the coarness of a past in the plandop of face to describe he would have you-ship's dear stong, don't pampied purchain whales.\"\n",
      "\n",
      "\"Mysting, sh \n",
      "\n",
      "Example 3 : outside evaluation :\n",
      "\n",
      "\n",
      "During his present short stay, Emma had barely seen him; but just enough   to feel that the first meeting was over, and to give her the impression   of his not being improved by the mixture of pique and pretension, now   spread over his air.  \n",
      "--------->\n",
      "                           \n",
      "          \n",
      "      \n",
      "                   least-For. The roved him a despect I do know\n",
      "that this distress--a solemn; \"What do, on these window of a patrios, and it was that stood in the strike of a let on Ahab, Ma-countened the partions; who had dreft to do the deck that the s \n",
      "\n",
      "Example 4 : outside evaluation :\n",
      "\n",
      "\n",
      "Poole swung the axe over his shoulder; the blow shook the building, and   the red baize door leaped against the lock and hinges. A dismal   screech, as of mere animal terror, rang from the cabinet. \n",
      "--------->\n",
      "She is othering the wonster,\n",
      "too fallong her than when the two dart and embraced and the great Ssurving, back, of the world, and had been sugged to the road was on the world; that sight and to say to be so now done. He\n",
      "                    \n",
      "      _Luist, and at framous work!\" cried quite struck rolle \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  hidden_size = 100\n",
    "  n_layers = 2\n",
    "  bi = True\n",
    "  # define model\n",
    "  model = TextGenerate(n_characters, hidden_size, n_characters, n_layers, bi)\n",
    "  print(\"Example 1 :\\n\")\n",
    "  # Pride and Prejudice - Jane Austen\n",
    "  print(generate(\"\\nThe tumult of her mind, was now painfully great. She knew not how \\\n",
    "  to support herself, and from actual weakness sat down and cried for \\\n",
    "  half-an-hour. \", 300, temperature=0.8),\"\\n\")\n",
    "  print(\"Example 2 :\\n\")\n",
    "\n",
    "  # Dracula - Bram Stoker\n",
    "  print(generate(\"\\nTo believe in things that you cannot. Let me illustrate. I heard once \\\n",
    "  of an American who so defined faith: 'that faculty which enables us to \\\n",
    "  believe things which we know to be untrue.' For one, I follow that man. \", 300, temperature=0.8),\"\\n\")\n",
    "\n",
    "  # outside evaluation\n",
    "  print(\"Example 3 : outside evaluation :\\n\")\n",
    "\n",
    "  # Emma - Jane Austen\n",
    "  print(generate(\"\\nDuring his present short stay, Emma had barely seen him; but just enough \\\n",
    "  to feel that the first meeting was over, and to give her the impression \\\n",
    "  of his not being improved by the mixture of pique and pretension, now \\\n",
    "  spread over his air.  \", 300, temperature=0.8),\"\\n\")\n",
    "  \n",
    "  print(\"Example 4 : outside evaluation :\\n\")\n",
    "\n",
    "  # The Strange Case Of Dr. Jekyll And Mr. Hyde - Robert Louis Stevenson\n",
    "  print(generate(\"\\nPoole swung the axe over his shoulder; the blow shook the building, and \\\n",
    "  the red baize door leaped against the lock and hinges. A dismal \\\n",
    "  screech, as of mere animal terror, rang from the cabinet. \", 300, temperature=0.8),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model, that have been trained on only few novels (small dataset compared to other text generator as chatGPT (even though chatGPT uses a transformer architecture, or self-attention architecture)), produces sentences almost understandable :). The author says that this model obtains a perplexity score of 93. Perplexity score gauges how surprised the model is when predicting a given output based on an input. A perplexity score of 1 means the model made a perfect prediction, while higher scores indicate greater uncertainty and weaker performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning\n",
    "\n",
    "Describing an image with text is a real challenge for a machine. Unlike us, who can effortlessly say what we see by relying on our experience, common sense, and context, an AI model struggles to connect pixels to words. The tricky part is that understanding an image isn’t just about recognizing objects—it also involves grasping their relationships, interpreting the scene, and putting everything into a meaningful sentence.\n",
    "\n",
    "For example, if we see a person walking their dog on the beach, we can instantly describe the scene, mentioning the weather, what the person is doing, or even the overall atmosphere. But for a model, it’s a whole different story—it first has to identify the visual elements (a person, a dog, sand, the ocean), then figure out how they interact, and finally generate a fluent and coherent description. And that’s no easy task.\n",
    "\n",
    "#### Early Approaches: CNN + RNN-Based Architectures\n",
    "Before attention mechanisms came along, image captioning models worked kind of like early machine translation systems, using an encoder-decoder setup. The idea was pretty straightforward:\n",
    "\n",
    " - A CNN (Convolutional Neural Network) scanned the image and turned it into a fixed-size set of features.\n",
    "\n",
    " - An RNN (Recurrent Neural Network) (or an LSTM/GRU) then took those features and generated a caption, word by word.\n",
    "\n",
    "Most models used a pre-trained CNN like ResNet or VGG to process the image and squash all the important details into a single vector. That vector was then handed over to an RNN, which tried to turn it into a meaningful sentence.\n",
    "\n",
    "The problem? Shrinking an entire image into one fixed vector meant losing a ton of important details. The model couldn’t always capture everything that mattered in the scene, making its descriptions a bit hit-or-miss. Plus, as the sentence got longer, you guessed it, the model struggled to keep track of context, often leading to vague or incomplete captions.\n",
    "\n",
    "#### A breakthrough : [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044), use of attention in image captioning\n",
    "\n",
    "The authors have published their code in an old Python version (2.6) on this [github from kelvinxu](https://github.com/kelvinxu/arctic-captions.git) if you want to have a look.\n",
    "I invite you to read this really good [github from sgrvinod](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git) which summarizes and implements very well the paper (in Python 3.6). This section contains explanations available on this github.\n",
    "\n",
    "The core idea behind attention in image captioning is that the model learns **where to look**. Indeed, while generation the caption, the model will focus on different parts of the picture to determine the most relevant one at the moment. Let's have a look at this picture : \n",
    "\n",
    "![exemple](img/sat.png)\n",
    "![exemple2](img/sat2.png)\n",
    "*Source : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git*\n",
    "\n",
    "As you can see above, on every picture there is a whitened part, which corresponds to where the model have focused his attention to generate the corresponding word.\n",
    "Let's have deeper look at the model's structure.\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "The Encoder takes an input image with three color channels and converts it into a smaller, more compact representation with learned feature channels. This compressed version captures all the essential details from the original image. Because the model deals with images, CNNs are used to encode the image. Indeed, over the years, they have gotten really good at recognizing objects across thousands of categories, so they naturally learn to capture the most important visual features. Moreover, there a several options that can be used, depending on the user's needs (e.g., VGG19 or ResNet-101).\n",
    "\n",
    "![exemple](img/cnn.png)\n",
    "*Source : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git*\n",
    "\n",
    "#### Decoder \n",
    "\n",
    "The decoder has one goal : take as an input the encoded image et generate the caption. In the case we decode the image without attention, the approach is to average the encoded image features accross all pixels to get a single representeation. To do so, we could use a RNN (like a LSTM) and generate each word based on the previous one.\n",
    "\n",
    "![no-attention](img/without-attention.png)\n",
    "*Source : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git*\n",
    "\n",
    "However, this is where the authors made the difference. Indeed, by using attention, the model generate the next word by focus on a part of the picture and not on what have been generated previously. For example, in the sentence 'A man holding a football', to generate the word 'man', the model focuses on the man. There, the approach is not to average accross all pixels but to use a weighted average. At each step, this weighted image representation can be combined with the previously generated word to help generate the next word in the sequence.\n",
    "\n",
    "![attention](img/decode.png)\n",
    "*Source : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git*\n",
    "\n",
    "#### Attention\n",
    "\n",
    "How would you figure out which part of an image is most important to focus on? You’d naturally consider what you've already described so far. This way, you can look at the image again and decide what needs to be mentioned next. For example, if you've just mentioned a man, the next logical step might be to describe that he’s holding a football.\n",
    "\n",
    "This is exactly what the Attention mechanism does—it keeps track of the words generated so far and dynamically focuses on the most relevant part of the image for the next word.\n",
    "\n",
    "![attention](img/attention11.png)\n",
    "*Source : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git*\n",
    "\n",
    "In the paper, two types of attention networks have been described and implemented. A deterministic 'soft' attention and stochastics 'hard' attention. We'll describe the difference between those two attention models. You just want to know that, basically, in the **soft attention** model, everything remains differentiable, so we can train the model end-to-end using standard backpropagation, while for the **hard attention** model, it is not. This allows the hard attention model to fixate on a sigle and highly relevant region at each step but the training is more unstable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Description of the two attention models**\n",
    "\n",
    "**Soft Attention**  \n",
    "\n",
    "At time step $ t $, we compute a score $ e_{t,i} $ for each image patch $ i $.  \n",
    "In general, this is given by:  \n",
    "\n",
    "$$\n",
    "e_{t,i} = w^{\\top} \\tanh(W_a \\, a_i + U_a \\, h_{t-1} + b_a)\n",
    "$$\n",
    "\n",
    "\n",
    "Intuitively, this score measures how relevant region $ i $ is based on the previous hidden state $ h_{t-1} $ and its own characteristics $ a_i $.  \n",
    "\n",
    "We then apply a *softmax* to these scores to obtain attention weights $ \\alpha_{t,i} $:  \n",
    "\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{k} \\exp(e_{t,k})}\n",
    "$$\n",
    " \n",
    "We compute a **context vector** $ z_t $ as a **weighted sum** of the patch feature vectors:  \n",
    "\n",
    "$$\n",
    "z_t = \\sum_{i=1}^{L} \\alpha_{t,i} \\, a_i\n",
    "$$\n",
    "\n",
    "The LSTM integrates $ z_t $ (along with the previously generated word) to produce the new hidden state $ h_t $.  \n",
    "Based on $ h_t $, the model predicts the next word.  \n",
    "\n",
    "> **Advantage:** Everything remains **differentiable**, so we can train the model end-to-end using standard backpropagation.  \n",
    "\n",
    "---\n",
    " **Hard Attention**  \n",
    "\n",
    "Instead of computing a **weighted sum** of patch features, **hard attention** **samples** a single patch $ i $ (randomly chosen) at each step $ t $, based on the probability $ \\alpha_{t,i} $.  \n",
    "\n",
    "The context vector $ z_t $ is then simply the feature vector $ a_i $ of the selected region.  \n",
    "\n",
    "Since this mechanism involves discrete sampling, it is **not differentiable**.  \n",
    "To train it, the authors use a reinforcement learning approach like **REINFORCE** (gradient estimation via sampling).  \n",
    "\n",
    "> **Advantage:** The model learns to **fixate** on a single, highly relevant region at each step.  \n",
    "> **Disadvantage:** Training is more unstable and requires specialized optimization heuristics.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "This is what the model looks like once everything is concatenated.\n",
    "\n",
    "![model](img/model.png)\n",
    "\n",
    "Once the **Encoder** processes the image, its encoded representation is used to initialize the **hidden state** $ h $ and **cell state** $ C $ of the LSTM Decoder.  \n",
    "\n",
    "At each decoding step:  \n",
    "\n",
    "1. The **encoded image** and the **previous hidden state** are used by the **Attention mechanism** to assign weights to different pixels.  \n",
    "2. The **previously generated word** and the **weighted sum** of the encoded features are then fed into the **LSTM Decoder** to generate the next word in the sequence.  \n",
    "\n",
    "Mathematically:  \n",
    "\n",
    "- The **attention scores** for each pixel $ i $ at time step $ t $ are computed as:  \n",
    "\n",
    "  $$\n",
    "  e_{t,i} = w^{\\top} \\tanh(W_a \\, a_i + U_a \\, h_{t-1} + b_a)\n",
    "  $$\n",
    "\n",
    "- The **attention weights** $  \\alpha_{t,i}  $ are obtained via a softmax function:  \n",
    "\n",
    "  $$\n",
    "  \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{k} \\exp(e_{t,k})}\n",
    "  $$\n",
    "\n",
    "- The **context vector** $ z_t $ is computed as the weighted sum of image features:  \n",
    "\n",
    "  $$\n",
    "  z_t = \\sum_{i=1}^{L} \\alpha_{t,i} \\, a_i\n",
    "  $$\n",
    "\n",
    "- Finally, the **LSTM Decoder** takes as input the **previously generated word** $ y_{t-1} $ and the **context vector** $ z_t $ to produce the next hidden state $ h_t $ and predict the next word $ y_t $.  \n",
    "\n",
    "  $$\n",
    "  h_t, C_t = \\text{LSTM}(y_{t-1}, z_t, h_{t-1}, C_{t-1})\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  y_t = \\text{Softmax}(W_o h_t + b_o)\n",
    "  $$\n",
    "\n",
    "This process repeats until the model generates an end-of-sequence token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search\n",
    "\n",
    "When generating text, a linear layer is used at the end of the decoder to assign a score to each word in the vocabulary. The easiest way to generate a sentence would be to **always pick the word with the highest score** at each step. But this approach, called greedy decoding, isn't great because each word you choose affects the rest of the sequence.  \n",
    "\n",
    "If you pick the wrong word early on, it can throw off the entire sentence, even if that first word had the highest individual score at the time.  \n",
    "\n",
    "Imagine you're writing a sentence, and the best possible sequence actually involves choosing the third-best word at the first step, then maybe the second-best word at the second step, and so on. A greedy approach would never find this optimal sequence.\n",
    "\n",
    "So, instead of locking in choices too soon, we'd like a way to consider multiple possible sentences and only commit once we've generated and evaluated full sequences.  \n",
    "\n",
    "This is where **Beam Search** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Beam Search Works**\n",
    "Instead of picking just one best word at each step, Beam Search keeps track of **multiple possible sequences** and only selects the best overall one at the end.\n",
    "\n",
    "1. At the first step, we choose the top k words (instead of just one).\n",
    "2. For each of these k words, we generate k possible second words.\n",
    "3. From all these k × k combinations, we pick the top k based on their total score.\n",
    "4. For each of these k second words, we generate k third words and repeat the process.\n",
    "5. We keep expanding the sequences until we reach the end of the sentence.\n",
    "6. Once we have k completed sentences, we pick the one with the highest overall score.\n",
    "\n",
    "This way, Beam Search finds a **more optimal sequence** instead of locking in a word too early and getting stuck with a bad choice.\n",
    "\n",
    "![beamsearch](img/beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how the model works, let's test it. We won't implement it here, because it is very long to do so, and training take lots of ressources. Indeed, the training uses [COCODATASET](https://cocodataset.org/#home), containing tens of thousands of pictures with their descriptions. To test the model, we will use again [srgvinod's github](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git), where we can find a trained model. However, because he used python 3.6 to develop the model, I had to do several modifications in the code and it might work a bit \"differently\". I copied his code to modify it on my github and so you can use it. I recommend using Google Colab if you don't have a GPU on your computer as it might not work on a CPU. This is just for you to test with your own pictures if you want to do so. \n",
    "\n",
    "I could manage to generate captions like this :\n",
    "\n",
    "![plane](img/planesky.png)\n",
    "\n",
    "\n",
    "![skater](img/skater.png)\n",
    "\n",
    "As you can see, results are very good, even though it might not work sometimes : \n",
    "\n",
    "![echec](img/captionchien.png)\n",
    "*Source : https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section is the script defining the model, written by [srgvinod](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning.git). It is quite long, but you can have a look at it, especially at the attention part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: Uses a pre-trained ResNet-101 model to extract visual features from an input image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Loads embedding layer with pre-trained embeddings.\n",
    "\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
    "\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ok it is your turn now to try and generate captions !**\n",
    "\n",
    "First, you need to download the two files [here](https://drive.google.com/drive/u/0/folders/189VY65I_n4RTpQnmLGj7IzVnOF6dmePC). Second, you need to put them in the \"model\" folder. If you are using google colab, just download the files and slide them here.\n",
    "![folderlocation](img/folderloc.png)\n",
    "![modeldansfolder](img/in_model.png)\n",
    "You can use the pictures I have used for this example or add your own images in the \"image_to_caption\" folder and have fun! You just have to change the name of the image in the next command (change *--img='image_to_caption/plane.jpg'*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(640x480)\n",
      "Caption has been generated : check 'caption.png' to have a look at it\n"
     ]
    }
   ],
   "source": [
    "!python3 notebook_tuto_captioning/caption.py \\\n",
    "      --img='image_to_caption/plane.jpg' \\\n",
    "      --model='model/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar' \\\n",
    "          --word_map='model/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json' --beam_size=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion : Why Attention Mechanism is a Game-Changer**\n",
    "\n",
    "The attention mechanism in image captioning and text generation is a big step forward because it lets models go beyond just recognizing objects—it actually helps them **grasp more abstract concepts and relationships within a scene, a sentence, or more**.  \n",
    "\n",
    "#### **More flexibility than object detectors**  \n",
    "Older methods relied on detecting specific objects, words, etc., first, but attention learns **what to focus on directly from data**. That means the model can highlight not just objects, but also patterns, textures, or even abstract ideas in an image.  \n",
    "\n",
    "#### **We can actually see what the model \"sees\"**\n",
    "Unlike most deep learning models that are black boxes, attention can be visualized through heatmaps. This helps us understand why the model generated a certain caption, making AI decisions more transparent.  \n",
    "\n",
    "#### **It could be useful in many fields**\n",
    "For instance, in the paper written by [**(H.Wang, *et al.*)**](https://ieeexplore.ieee.org/abstract/document/8852343?casa_token=msDOIuCblPIAAAAA:wVxucaHRLnVSHNZB-J1PGMqK0U_tI6tgKnqsi2H_saLJay4wHfJkS6RFLm6gOWAlzos9tVo9Nm0), they managed to summarize NBA games from statistical tables. For this table :\n",
    "\n",
    "![table](img/tables.png) \n",
    "*Source : https://ieeexplore.ieee.org/abstract/document/8852343?casa_token=msDOIuCblPIAAAAA:wVxucaHRLnVSHNZB-J1PGMqK0U_tI6tgKnqsi2H_saLJay4wHfJkS6RFLm6gOWAlzos9tVo9Nm0*\n",
    "\n",
    "The model managed to generate this caption : *The Atlanta Hawks (46 - 12) defeated the Orlando Magic (19 - 41) 95 - 88 on Wednesday at Philips Arena in Atlanta. The Hawks got off to a quick start in this one, out - scoring the Magic 28 - 16 in the first quarter alone. Along with the quick start, the Hawks were able to out - score the Magic 28 - 21 in the third quarter, while the Hawks were able to coast to a victory in front of their home crowd. The Hawks were also able to out - rebound the Magic 42 - 40, giving them enough of an advantage to secure the victory in front of their home crowd. The Hawks were led by the duo of Victor Oladipo and Nikola Vucevic. Oladipo went 8 - for - 18 from the field and 2 - for - 5 from the three - point line to score a game - high of 19 points, while also adding six assists and two steals. He’s now averaging 17 points and 6 rebounds on the year. Jeff Teague also had a solid showing, finishing with 17 points (6 - 15 FG, 1 - 4 3Pt, 4 - 4 FT), seven assists and two steals. It was his second double - double in a row, a stretch where he’s averaging 24 points and 12 rebounds over that span. Coming off the bench, Kyle Korver had a solid showing as well, finishing with 19 points (2 - 5 FG, 2 - 5 3Pt, 2 - 2 FT), three rebounds and two blocked shots. It was his second double - double in a row, a stretch where he’s averaging 17 points and 12 rebounds. The only other Magic player to reach double figures in points was Victor Oladipo, who chipped in with 19 points (8 - 18 FG, 2 - 5 3Pt, 1 - 2 FT) and six assists. The Magic’s next game will be at home against Detroit Pistons on Friday, while the Magic will be at home against the Detroit Pistons on Friday.*\n",
    "\n",
    "It is quite impressive, but there many more fields. Indeed, this **encoder-decoder + attention** approach is modular, meaning it could be adapted for other AI tasks like:  \n",
    "- **Medical imaging**  \n",
    "- **Video description**  \n",
    "- **Advanced AI storytelling**  \n",
    "\n",
    "#### **Bottom line?**\n",
    "Attention **helps models \"look\" at images more like humans do**, leading to **smarter and more detailed descriptions**. Plus, it’s way more transparent than traditional AI methods, making it **easier to trust and improve**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "[[1] Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, attend and tell: neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37 (ICML'15). JMLR.org, 2048–2057.](https://dl.acm.org/doi/10.5555/3045118.3045336)\n",
    "\n",
    "Github's implementation : [arctic-captions](https://github.com/kelvinxu/arctic-captions)\n",
    "\n",
    "[[2] H. Wang, W. Zhang, Y. Zhu and Z. Bai, \"Data-to-Text Generation with Attention Recurrent Unit,\" 2019 International Joint Conference on Neural Networks (IJCNN), Budapest, Hungary, 2019, pp. 1-8, doi: 10.1109/IJCNN.2019.8852343. keywords: {Decoding;Logic gates;Task analysis;Computational modeling;Mathematical model;Standards;Context modeling;Long Short-Term Memory (LSTM);Attention Recurrent Unit (ARU);DoubleAtten}](https://ieeexplore.ieee.org/abstract/document/8852343?casa_token=msDOIuCblPIAAAAA:wVxucaHRLnVSHNZB-J1PGMqK0U_tI6tgKnqsi2H_saLJay4wHfJkS6RFLm6gOWAlzos9tVo9Nm0)\n",
    "\n",
    "sgrvinod's github : [a-PyTorch-Tutorial-to-Image-Captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
